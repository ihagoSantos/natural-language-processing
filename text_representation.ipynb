{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqyGh95BPDDNkCN0gECCy9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihagoSantos/natural-language-processing/blob/main/text_representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGeP-olGHtGJ",
        "outputId": "e00eb0bd-3b03-4d0e-d74c-c53011cf3b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.6)\n",
            "Requirement already satisfied: unidecode==1.3.8 in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Collecting scikit-learn==1.3.1\n",
            "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1) (3.5.0)\n",
            "Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.1\n"
          ]
        }
      ],
      "source": [
        "# Instalação dos pacotes necessários\n",
        "!pip install nltk==3.8.1\n",
        "!pip install unidecode==1.3.8\n",
        "!pip install scikit-learn==1.3.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação das bibliotecas padrão\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "# Importação das bibliotecas de terceiros\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Download do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Desativando warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Pacotes importados com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFohvZYYH9Bv",
        "outputId": "1792962a-2ba8-400d-9422-ca8ff76aa0af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pacotes importados com sucesso!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declara funções utilitárias\n",
        "def formata_msg(nivel, msg):\n",
        "  timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "  return f'[{nivel}] {timestamp} - {msg}'\n",
        "\n",
        "print(formata_msg(\"INFO\", 'Funções utilitárias prontas para uso'))\n",
        "print(formata_msg(\"INFO\", f'Versão do python: {sys.version}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8RAD9ITJOlC",
        "outputId": "36694fff-9d32-4c22-ee30-48f9b7360bb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2024-12-05 09:34:46 - Funções utilitárias prontas para uso\n",
            "[INFO] 2024-12-05 09:34:46 - Versão do python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição do Corpus"
      ],
      "metadata": {
        "id": "vLasLm4gKA5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentos =\\\n",
        "[\n",
        "    # No meio do caminho - Carlos Drumond de Andrade\n",
        "    'No meio do caminho tinha uma pedra\\n'\\\n",
        "    'tinha uma pedra no meio do caminho\\n'\\\n",
        "    'tinha uma pedra\\n'\\\n",
        "    'no meio do caminho tinha uma pedra.\\n'\\\n",
        "    'Nunca me esquecerei desse acontecimento\\n'\\\n",
        "    'na vida de minhas reti nas tão fatigadas.\\n'\\\n",
        "    'Nunca me esquecerei que no meio do caminho\\n'\\\n",
        "    'tinha uma pedra\\n'\\\n",
        "    'tinha uma pedra no meio do caminho\\n'\\\n",
        "    'no meio do caminho tinha uma pedra.'\n",
        "    ,\n",
        "    # Quadrilha - Carlos Drumond de Andrade\n",
        "    'João amava Teresa que amava Raimundo\\n'\\\n",
        "    'que amava Maria que amava Joaquim que amava Lili,\\n'\\\n",
        "    'que não amava ninguém.\\n'\\\n",
        "    'João foi para os Estados Unidos, Teresa para o convento,\\n'\\\n",
        "    'Raimundo morreu de desastre, Maria ficou pra titia,\\n'\\\n",
        "    'Joaquim suicidou-se e Lili casou-se com J. Pinto Fernandes\\n'\\\n",
        "    'que não tinha entrado na história.'\n",
        "]\n",
        "\n",
        "print(formata_msg('INFO', f\"Documentos:\\n{documentos}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkapfDYeJ5kW",
        "outputId": "aac34d7e-94f0-4292-c3f2-b805b5e0a993"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2024-12-05 09:41:22 - Documentos:\n",
            "['No meio do caminho tinha uma pedra\\ntinha uma pedra no meio do caminho\\ntinha uma pedra\\nno meio do caminho tinha uma pedra.\\nNunca me esquecerei desse acontecimento\\nna vida de minhas reti nas tão fatigadas.\\nNunca me esquecerei que no meio do caminho\\ntinha uma pedra\\ntinha uma pedra no meio do caminho\\nno meio do caminho tinha uma pedra.', 'João amava Teresa que amava Raimundo\\nque amava Maria que amava Joaquim que amava Lili,\\nque não amava ninguém.\\nJoão foi para os Estados Unidos, Teresa para o convento,\\nRaimundo morreu de desastre, Maria ficou pra titia,\\nJoaquim suicidou-se e Lili casou-se com J. Pinto Fernandes\\nque não tinha entrado na história.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-processamento"
      ],
      "metadata": {
        "id": "A0c_tJTnLf66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atividade\n",
        "1) Escreva um método que realiza o pré-processamento da lista de documentos.\\\n",
        "O Método deve, para cada documento:\n",
        "- Tokenizar cada palavra\n",
        "- Remover stopwords\n",
        "- Remover números\n",
        "- Remover pontuações\n",
        "- Remover acentos"
      ],
      "metadata": {
        "id": "COPL70uYLk5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processa_texto(texto):\n",
        "  \"\"\"\n",
        "  Pré-processa o texto fornecido realizando várias etapas de limpeza.\n",
        "\n",
        "  Etapas:\n",
        "  1 - Tokeniza o texto\n",
        "  2 - Converte os tokens para minúsculo\n",
        "  3 - Remove stopwords em português\n",
        "  4 - Remove números dos tokens\n",
        "  5 - Exclui tokens que são pontuações\n",
        "  6 - Remove acentuações dos tokens\n",
        "\n",
        "  Parâmetros:\n",
        "  texto (str): O texto a ser processado\n",
        "\n",
        "  Retorna:\n",
        "  lista: Lista de tokens pré-processados\n",
        "  \"\"\"\n",
        "  padrao = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
        "  tokens_preprocessados = re.findall(padrao, texto)\n",
        "  portugues_stops = stopwords.words('portuguese')\n",
        "  resultado = []\n",
        "\n",
        "  # Tokeniza o texto usando um padrão para capturar as palavras e pontuações\n",
        "  padrao = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
        "  tokens_preprocessados = re.findall(padrao, texto)\n",
        "\n",
        "  # Converte tokens para minusculos para padronizar a captalização\n",
        "  tokens_preprocessados = [token.lower() for token in tokens_preprocessados]\n",
        "\n",
        "  # Remove stopwords para reduzir o conjunto de tokens a palavras significativas\n",
        "  portugues_stops = stopwords.words('portuguese')\n",
        "  tokens_preprocessados = [token for token in tokens_preprocessados if token not in portugues_stops]\n",
        "\n",
        "  # Remove números, pois geralmente não contribuem para o significado do texto\n",
        "  tokens_preprocessados = [re.sub(r'\\d+', '', token) for token in tokens_preprocessados if re.sub(r'\\d+', '', token)]\n",
        "\n",
        "  # Exclui tokens que são pontuações, pois raramente são úteis para a análise de texto.\n",
        "  tokens_preprocessados = [token for token in tokens_preprocessados if token not in string.punctuation]\n",
        "\n",
        "  # Remove acentuações para padronizar os tokens\n",
        "  tokens_preprocessados = [unidecode(token) for token in tokens_preprocessados]\n",
        "\n",
        "  return ' '.join(tokens_preprocessados)\n"
      ],
      "metadata": {
        "id": "LJRGVdJqLaTg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos_preprocessados = [pre_processa_texto(poema) for poema in documentos]\n",
        "print('INFO', f'Documentos preprocessados:\\n{documentos_preprocessados}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBE5lSltOLDH",
        "outputId": "366885e6-4cf3-499b-c1d9-63240fc72268"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO Documentos preprocessados:\n",
            "['meio caminho pedra pedra meio caminho pedra meio caminho pedra nunca esquecerei desse acontecimento vida reti tao fatigadas nunca esquecerei meio caminho pedra pedra meio caminho meio caminho pedra', 'joao amava teresa amava raimundo amava maria amava joaquim amava lili amava ninguem joao estados unidos teresa convento raimundo morreu desastre maria ficou pra titia joaquim suicidou lili casou j pinto fernandes entrado historia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representação Textual\n",
        "\n",
        "## Bag of Words\n",
        "\n",
        "Faça um CountVectorizer nos documentos da variável documentos considerando binary = True"
      ],
      "metadata": {
        "id": "tjbtbPrBRf87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aplica_bow(documentos_preprocessados):\n",
        "  \"\"\"\n",
        "  Analisa documentos usando BoW e retorna o vocabulário extraído e um Dataframe contendo os valores do BoW para cada termo do documento.\n",
        "\n",
        "  Parâmetros:\n",
        "  - documentos_preprocessados: Lista de documentos pre-processados\n",
        "\n",
        "  Retorna:\n",
        "  - Um dicionário contendo o vocabulário extraído e o DataFrame dos valores do BoW.\n",
        "  \"\"\"\n",
        "\n",
        "  # Inicia Vectorizer\n",
        "  vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "  # Transforma os documentos em uma matriz binária\n",
        "  bow_matrix = vectorizer.fit_transform(documentos_preprocessados)\n",
        "\n",
        "  # Obtem vocabulário extraído\n",
        "  vocabulario = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # Cria um DataFrame para os valores do BoW\n",
        "  df = pd.DataFrame(\n",
        "      bow_matrix.T.todense(),\n",
        "      index=vocabulario,\n",
        "      columns = [f\"doc{i+1}\" for i in range(bow_matrix.shape[0])]\n",
        "  )\n",
        "  return {\n",
        "      \"vocabulario\": vocabulario,\n",
        "      \"bow_dataframe\": df\n",
        "  }"
      ],
      "metadata": {
        "id": "QJQ4cq97OWeI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_dict = aplica_bow(documentos_preprocessados)\n",
        "print(formata_msg('INFO', f'Vocabulário BoW:\\n{bow_dict[\"vocabulario\"]}'))\n",
        "print(formata_msg('INFO', f'DataFrame BoW:\\n{bow_dict[\"bow_dataframe\"]}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nIWqdZzSJc4",
        "outputId": "0187ce17-384b-4502-8009-268532fb0a41"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2024-12-05 10:14:35 - Vocabulário BoW:\n",
            "['acontecimento' 'amava' 'caminho' 'casou' 'convento' 'desastre' 'desse'\n",
            " 'entrado' 'esquecerei' 'estados' 'fatigadas' 'fernandes' 'ficou'\n",
            " 'historia' 'joao' 'joaquim' 'lili' 'maria' 'meio' 'morreu' 'ninguem'\n",
            " 'nunca' 'pedra' 'pinto' 'pra' 'raimundo' 'reti' 'suicidou' 'tao' 'teresa'\n",
            " 'titia' 'unidos' 'vida']\n",
            "[INFO] 2024-12-05 10:14:35 - DataFrame BoW:\n",
            "               doc1  doc2\n",
            "acontecimento     1     0\n",
            "amava             0     1\n",
            "caminho           1     0\n",
            "casou             0     1\n",
            "convento          0     1\n",
            "desastre          0     1\n",
            "desse             1     0\n",
            "entrado           0     1\n",
            "esquecerei        1     0\n",
            "estados           0     1\n",
            "fatigadas         1     0\n",
            "fernandes         0     1\n",
            "ficou             0     1\n",
            "historia          0     1\n",
            "joao              0     1\n",
            "joaquim           0     1\n",
            "lili              0     1\n",
            "maria             0     1\n",
            "meio              1     0\n",
            "morreu            0     1\n",
            "ninguem           0     1\n",
            "nunca             1     0\n",
            "pedra             1     0\n",
            "pinto             0     1\n",
            "pra               0     1\n",
            "raimundo          0     1\n",
            "reti              1     0\n",
            "suicidou          0     1\n",
            "tao               1     0\n",
            "teresa            0     1\n",
            "titia             0     1\n",
            "unidos            0     1\n",
            "vida              1     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n",
        "1) Faça o TF-IDF Vectorizer nos documentos sem alterar nenhum parâmetro"
      ],
      "metadata": {
        "id": "EyEjrtYfTm0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aplica_tf_idf(documentos_preprocessados):\n",
        "  \"\"\"\n",
        "  Analisa documentos usando TF-IDF e retorna o vocabulário extraído e um\n",
        "  dataframe contendo os valores TF-IDF para cada termo do documento\n",
        "\n",
        "  Parâmetros:\n",
        "  - documentos_preprocessados: Lista de documentos pre-processados\n",
        "\n",
        "  Retorna:\n",
        "  - Um dicionário contendo o vocabulário extraído e o DataFrame dos valores do TF-IDF.\n",
        "  \"\"\"\n",
        "\n",
        "  # Inicializa o TfidfVectorizer\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Transforma documento em uma matriz TF-IDF\n",
        "  tfidf_matrix = vectorizer.fit_transform(documentos_preprocessados)\n",
        "\n",
        "  # Obtem vocabulário extraído\n",
        "  vocabulario = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # Cria um DataFrame para os valores de TF-IDF\n",
        "  df = pd.DataFrame(\n",
        "      tfidf_matrix.T.todense(),\n",
        "      index=vocabulario,\n",
        "      columns = [f\"doc{i+1}\" for i in range(tfidf_matrix.shape[0])]\n",
        "  )\n",
        "\n",
        "  return {\n",
        "      \"vocabulario\": vocabulario,\n",
        "      \"tfidf_dataframe\": df\n",
        "  }"
      ],
      "metadata": {
        "id": "A_Pxp_WqTBBd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dict = aplica_tf_idf(documentos_preprocessados)\n",
        "print(formata_msg('INFO', f'Vocabulário tfidf:\\n{tfidf_dict[\"vocabulario\"]}'))\n",
        "print(formata_msg('INFO', f'DataFrame tfidf:\\n{tfidf_dict[\"tfidf_dataframe\"]}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RhdR5pkUq2r",
        "outputId": "c32cd15e-d588-4587-bfae-4ce8b62ee656"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2024-12-05 10:23:02 - Vocabulário tfidf:\n",
            "['acontecimento' 'amava' 'caminho' 'casou' 'convento' 'desastre' 'desse'\n",
            " 'entrado' 'esquecerei' 'estados' 'fatigadas' 'fernandes' 'ficou'\n",
            " 'historia' 'joao' 'joaquim' 'lili' 'maria' 'meio' 'morreu' 'ninguem'\n",
            " 'nunca' 'pedra' 'pinto' 'pra' 'raimundo' 'reti' 'suicidou' 'tao' 'teresa'\n",
            " 'titia' 'unidos' 'vida']\n",
            "[INFO] 2024-12-05 10:23:02 - DataFrame tfidf:\n",
            "                   doc1     doc2\n",
            "acontecimento  0.086066  0.00000\n",
            "amava          0.000000  0.69282\n",
            "caminho        0.516398  0.00000\n",
            "casou          0.000000  0.11547\n",
            "convento       0.000000  0.11547\n",
            "desastre       0.000000  0.11547\n",
            "desse          0.086066  0.00000\n",
            "entrado        0.000000  0.11547\n",
            "esquecerei     0.172133  0.00000\n",
            "estados        0.000000  0.11547\n",
            "fatigadas      0.086066  0.00000\n",
            "fernandes      0.000000  0.11547\n",
            "ficou          0.000000  0.11547\n",
            "historia       0.000000  0.11547\n",
            "joao           0.000000  0.23094\n",
            "joaquim        0.000000  0.23094\n",
            "lili           0.000000  0.23094\n",
            "maria          0.000000  0.23094\n",
            "meio           0.516398  0.00000\n",
            "morreu         0.000000  0.11547\n",
            "ninguem        0.000000  0.11547\n",
            "nunca          0.172133  0.00000\n",
            "pedra          0.602464  0.00000\n",
            "pinto          0.000000  0.11547\n",
            "pra            0.000000  0.11547\n",
            "raimundo       0.000000  0.23094\n",
            "reti           0.086066  0.00000\n",
            "suicidou       0.000000  0.11547\n",
            "tao            0.086066  0.00000\n",
            "teresa         0.000000  0.23094\n",
            "titia          0.000000  0.11547\n",
            "unidos         0.000000  0.11547\n",
            "vida           0.086066  0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCWRXFL4Uwed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}